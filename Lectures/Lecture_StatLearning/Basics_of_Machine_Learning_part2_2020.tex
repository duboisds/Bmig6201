\documentclass[11pt]{article}    
    \usepackage[breakable]{tcolorbox}
    \tcbset{nobeforeafter} % prevents tcolorboxes being placing in paragraphs
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
     \graphicspath{{../../figures/}}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Lecture: Basics of Statistical Learning\\ BMIG 5003}   
    \author{Horacio G\'omez-Acevedo,PhD\\ Department of Biomedical Informatics\\UAMS}
 
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \newcommand{\prompt}[4]{
        \llap{{\color{#2}[#3]: #4}}\vspace{-1.25em}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{statistical-learning}{%
\section{Foundations }\label{statistical-learning}}


  

    Let's suppose we observe a quatitative response \(Y\) and \(p\)
different predictors \(X_1,\ldots, X_p\), which can be written in the
form \begin{equation*}
Y= f(X) + \varepsilon
\end{equation*} We assume that the function \(f\) is \textbf{fixed but
unknown}, and \(\varepsilon\) is a \textbf{random error term}, which is
independent of \(X\) and has mean zero (i.e.~its expected value
\(E (\varepsilon) =0\)).

\textbf{Statistical Learning} refers to a set of approaches for
estimating \(f\), for the purpose of either predict future responses or
make inferences about inner relationships within the model predictors.

    \hypertarget{prediction}{%
\subsubsection{Prediction}\label{prediction}}

Let's suppose that a set of input (predictors) is readily available, but
for some financial or practical constrains we cannot estimate the output
\(Y\). We can predict \(Y\) using \begin{equation*}
\hat{Y} = \hat{f}(X),
\end{equation*} where \(\hat{f}\) represents an estimate for \(f\) and
\(\hat{Y}\) respresent the resulting prediction for \(Y\).

\begin{quote}
Note that this is a very general setting, for instance in (multi)linear
regression, we cannot make predictions outside certain regions.
\end{quote}

\hypertarget{how-accurate-is-haty}{%
\paragraph{\texorpdfstring{How accurate is
\(\hat{Y}\)?}{How accurate is \textbackslash{}hat\{Y\}?}}\label{how-accurate-is-haty}}

The accuracy of \(\hat{Y}\) as a prediction for \(Y\) depends on two
quatitites + reducible error + irreducible error

The reducible error reflects the leverage that modeler has to pick a
\emph{better} statistical technique for estimation. Even in the case
when we have a perfect estimate of \(f\) (i.e. \(\hat{Y}=f(X)\)), we
will have \begin{equation*}
|\hat{Y}-Y| = \varepsilon
\end{equation*} since \(\varepsilon\) does not depend on \(X\). Thus, we
cannot reduce the error introduced by \(\varepsilon\) no matter how well
our estimates are.

Recall that if you have a random variable \(Z\) (with \(E(|Z|)<\infty\)
and \(\mathrm{Var}(Z)<\infty\)), the following relationships hold

\[E(Z^2)= \mathrm{Var}(Z) + (E(Z))^2\] and
\[\mathrm{Var}(aZ+b)= a^2\mathrm{Var}(Z)\]

Now if we consider for a moment that both \(\hat{f}\) and \(X\) are
fixed (\(f\) was already fix but unknown), we have that the average
(expected value) of the squared difference is

\begin{equation}
\begin{split}
E(Y-\hat{Y})^2&= E(f(X)+ \varepsilon -\hat{f}(X))^2= \mathrm{Var}(f(X)-\hat{f}(X)+ \varepsilon ) + \left(E(f(X)-\hat{f}(X)+\varepsilon) \right)^2 \\
&= \mathrm{Var}(\varepsilon) + \mathrm{Var}(f(X)-\hat{f}(X))+ \left( E(f(X)-\hat{f}(X)) + E(\varepsilon) \right)^2\\
&= \color{red}{\mathrm{Var}(\varepsilon)}+\color{blue}{E\left( f(X)-\hat{f}(X)\right)^2} 
\end{split}
\end{equation} The reducible error is colored in blue and the
irreducible error is in red.

Thus, the accuracy will be (lower) bounded by
\(\mathrm{Var}(\varepsilon)\), but this bound is almost never known in
practice.

    \hypertarget{inference}{%
\subsubsection{Inference}\label{inference}}

In this case we are mainly interested in determined the way that \(Y\)
is affected by changing \(X_1,\ldots,X_p\). This means trying to
understand the relationship between the output and the predictors. For
instance we try to answer questions like

\begin{itemize}
\tightlist
\item
  Which predictors are more important?
\item
  What relationship exist between the response and each of the
  predictors?
\item
  Is the relationship between \(Y\) and predictor \(X_i\) linear, or
  should it be nonlinear?
\end{itemize}

    \hypertarget{estimation-of-f}{%
\subsection{\texorpdfstring{Estimation of
\(f\)}{Estimation of f}}\label{estimation-of-f}}

The observations at hand are called \textbf{training data}, because we
will use those observations to find an estimate \(\hat{f}\). Let
\(x_{ij}\) represents the value of the predictor \(j\) for the \(i\)th
observation (\(i\in \{1,\ldots,n\}, j\in \{1,\ldots,p\}\), and the
response variable \(y_i\) for the \(i\)th observation. Our dataset looks
something like this

\begin{equation}
\left\{
\left(
\left( 
\begin{matrix}
x_{11}\\
x_{12}\\
\vdots \\
x_{1p}
\end{matrix}\right),
y_1 \right), 
\left(
\left( 
\begin{matrix}
x_{21}\\
x_{22}\\
\vdots \\
x_{2p}
\end{matrix}\right),
y_2 \right),
\ldots,
\left(
\left( 
\begin{matrix}
x_{n1}\\
x_{n2}\\
\vdots \\
x_{np}
\end{matrix}\right),
y_n
\right)
\right\}
\end{equation}

    \hypertarget{example.}{%
\paragraph{Example.}\label{example.}}

We will consider the dataset \emph{income2.csv} that consists of
\(n=30\) datapoints about seniority, income and level of education. We
will apply a statistical learning method to find a function \(\hat{f}\)
that approximates that approximates \(Y\) (i.e., \(Y \sim f(\hat{X})\)
for any observation \((X,Y)\)).

\hypertarget{parametric-method}{%
\subparagraph{Parametric Method}\label{parametric-method}}

Parametric methods are those methods in which we need to find certain
parameters for the estimation of \(f\).

\begin{itemize}
\tightlist
\item
  We need to select the functional form of \(f\).
\end{itemize}

Traditionally, we apply the condition that \(f\) is linear

\begin{equation}
f(X) = \beta_0+ \beta_1 X_1 + \cdots+ \beta_p X_p
\end{equation}

\begin{itemize}
\tightlist
\item
  We need to use a procedure that uses the training data to \textbf{fit}
  or \textbf{train} the model
\end{itemize}

In the previous setup, this would mean to find values for the parameters
\(\beta_0, \beta_1,\ldots,\beta_p\) such that \begin{equation*}
Y \sim \beta_0+ \beta_1 X_1 + \cdots+ \beta_p X_p
\end{equation*}

One procedure that can be used is \textbf{(ordinary) least squares}.

    Going back to our example, if \(X_1\) represents \emph{seniority} and
\(X_2\) \emph{years of education} and \(Y\) the \emph{annual income},
let's assume for a moment that we know the exact function \(f\) (the
graph (surface) of \(f\) depicted in blue, and the red dots represent
each of the observations from our dataset)

\begin{figure}[h!]
    \centering
    \includegraphics{2.3.png}
    \caption{The ground truth function $f$ in blue}
\end{figure}

    If we apply (ordinary) least squares, we have to fit a linear model of
the form \begin{equation*}
\mathrm{income} \approx \beta_0 + \beta_1 \times\mathrm{education} + \beta_2 \times \mathrm{seniority}  
\end{equation*}

And the linear model fit look like

\begin{figure}[h!]
    \centering
    \includegraphics{2.4.png}
    \caption{The linear model approximation }
\end{figure}

Note that \(\hat{f}\) does not match the unknown function \(f\), and we
may think is other models that \emph{look more like} \(f\) by adding
certain complexity (e.g., adding quadratic terms). More complex models
tend to \textbf{overfit} the data (i.e., they follow too close the
errors and performed poorly when predicting unseen data).

    \hypertarget{non-parametric-methods}{%
\paragraph{Non-parametric methods}\label{non-parametric-methods}}

These methods do not make the assumption about the form of the function
\(f\). Instead they look for an estimate of \(f\) that gets as close to
the points as possible without given (hyper)surfaces that are too rough
or wiggly.

One of this methodologies for instance is the \emph{thin plate splines}
that tries to adjust a surface so that \emph{bending energy} of the
points is optimized.

\begin{figure}[h!]
    \centering
    \includegraphics{2.6.png}
    \caption{The approximation with thin plate splines of $f$}
\end{figure}

Two methodologies that are worth mentioning for non-parametric test are

\begin{itemize}
\tightlist
\item
  \emph{loess} (local regression) where a parameter determines the size
  of the window in which the local regression will be applied.
\item
  \emph{restricted cubic splines} in which the knots of the spline are
  driven by the data itself.
\end{itemize}

    \hypertarget{model-interpretability-or-prediction-accuracy}{%
\subsubsection{Model interpretability or prediction
accuracy}\label{model-interpretability-or-prediction-accuracy}}

Whereas models with larger number of parameters tend to improve accuracy
it comes at the expense of model interpretability.

    \hypertarget{model-accuracy}{%
\subsection{Model Accuracy}\label{model-accuracy}}

First, there is not a single statistical method that can be accurate
under all the circumstances.

One way to check accuracy is by measuring the \textbf{quality of fit},
which means to determine how well the predictions match the observed
data.

In (multi)linear regression, one commonly-used measure is the
\textbf{MEAN SQUARED ERROR (MSE)}

\begin{equation}
\mathrm{MSE}= \frac{1}{n}\sum_{i=1}^n (y_i -\hat{f}(x_i))^2 = \frac{1}{n} \sum_{i=1}^n e_i^2,
\label{eq:mse}
\end{equation} where \(e_i\) are called the residuals.


 \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
Note that MSE is sometimes divided by \(n-2\) instead of \(n\)
\end{tcolorbox}

For instance, in the case of the linear regression with one predictor
variable \(Y=\beta_0+ \beta_1 X + \varepsilon\), where
\(\mathrm{Var}(\varepsilon)=\sigma^2\), we have that
\(\frac{1}{n-2} \sum_{i=1}^n e_i^2\) is an unbiassed estimator of
\(\sigma^2\).

\hypertarget{training-and-test-mse}{%
\subsubsection{Training and test MSE}\label{training-and-test-mse}}

We will refer to the MSE obtained from the observations as the
\textbf{training MSE}. Thus, if we have our \emph{training observations}
\(\{ (x_1,y_1),\ldots,(x_n,y_n) \}\), we obtain the estimate \(\hat{f}\)
by using say multlinear regression. Then, the training MSE is suppose to
be small for \(\hat{f}\), but can still use another more sophisticated
model and get smaller MSE (say by restricted cubic splines).

However, if we would prefer to know \emph{how well the model perfoms for
a set of previously unseen observations} we need to quantify the
\textbf{test MSE}. Let's suppose that we have \emph{new} observations
\(\{ (\tilde{x_1},\tilde{y_1}),\ldots,(\tilde{x_m},\tilde{y_m})\}\),
then we would like to select the method that minimizes equation
\ref{eq:mse} when applied for our new set of observations.

    If we consider a plot comparing the MSE with model flexibility
(technically called \emph{degrees of freedom}), we observe two
fundamental properties

\begin{itemize}
\tightlist
\item
  test MSE has a U-shape
\item
  training MSE is monotonically decreases as the model complexity
  increases.
\end{itemize}

The simulated data from \(f\) and three estimates are given (red linear
regression and blue and green smoothing splines). The training MSE is
shown in gray, and test MSE in red. The squares represent the values for each of those
estimates.

\begin{figure}[h!]
    \centering
    \includegraphics{2.9.png}
\end{figure}

In practice, an estimation of test MSE is much more difficult, but there
are approaches that can be used to determine (namely,
\textbf{cross-validation}).

    \hypertarget{bias-variance-trade-off}{%
\subsection{Bias-Variance trade-off}\label{bias-variance-trade-off}}

Let's suppose we have a given value \(x_0\), then the expected value of
the test MSE can be decomposed into:

\begin{itemize}
\tightlist
\item
  variance of \(\hat{f}(x_0)\),
\item
  squared bias of \(\hat{f}(x_0)\),
\item
  variance of the error terms \(\varepsilon\).
\end{itemize}

Recall that the \textbf{bias} of the estimator \(\hat{\Theta}\) of the
random variable \(\Theta\) is defined as

\begin{equation}
\mathrm{Bias}(\hat{\Theta})= E(\hat{\Theta}) - \Theta 
\end{equation}

The estimator \(\hat{\Theta}\) is called unbiassed when
\(\mathrm{Bias}(\hat{\Theta})=0\).

In mathematical terms, test MSE is defined as an average (expected
value) of square differences between the new values and their estimates,
that is \(E[(y_0-\hat{f}(x_0))^2]\).

Note that \(f\) is \emph{fixed} and theoretically known, so even when we
have a new dataset \((x_0,y_0)\), their values are known without any
error, thus \(E(f(x_0))=f(x_0)\) and \(\mathrm{Var}(f(x_0))=0\).

\begin{equation}
\label{eq:bias-variance}
\begin{split}
E\left[ (y_0- \hat{f}(x_0))^2 \right] &= E[ (f(x_0)+ \varepsilon - \hat{f}(x_0))^2 ]\\
&= \mathrm{Var}(f(x_0) + \varepsilon - \hat{f}(x_0)) + \left[ E(f(x_0)+\varepsilon -\hat{f}(x_0))\right]^2 \\
&= \mathrm{Var}(f(x_0))+ \mathrm{Var}(\hat{f}(x_0)) + \mathrm{Var}(\varepsilon) + 
\left[ E(f(x_0)-\hat{f}(x_0))+ E(\varepsilon)\right]^2\\
&= \mathrm{Var}(\hat{f}(x_0)) +\mathrm{Var}(\varepsilon) + \left[ E(f(x_0)-\hat{f}(x_0))\right]^2\\
&= \mathrm{Var}(\hat{f}(x_0)) +\mathrm{Var}(\varepsilon) + \left[ f(x_0) - E(\hat{f}(x_0))\right]^2\\
&= \mathrm{Var}(\hat{f}(x_0)) +\mathrm{Var}(\varepsilon) + \left[\mathrm{Bias}(\hat{f}(x_0)) \right]^2
\end{split}
\end{equation}

What does it mean?

The \emph{variance} refers to the amount by which \(\hat{f}\) would
change if we estimated it using a different training set. Intuitively,
if our model is simple (say linear regression) changing our training
dataset won't change the line that much, but if we have a model that
follows closely the training set, replacing a point will increase the
variance significantly.

On the other hand, \emph{bias} refers to the error that is introduced by
approximating a real-life problem (say highly non-linear) with a simpler
model say a linear one.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n+nb}{print}\PY{p}{(}\PY{n+nv+vm}{\PYZus{}\PYZus{}doc\PYZus{}\PYZus{}}\PY{p}{)}

\PY{c+c1}{\PYZsh{} Author: Mathieu Blondel}
\PY{c+c1}{\PYZsh{}         Jake Vanderplas}
\PY{c+c1}{\PYZsh{} License: BSD 3 clause}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}

\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{linear\PYZus{}model} \PY{k}{import} \PY{n}{Ridge}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{preprocessing} \PY{k}{import} \PY{n}{PolynomialFeatures}
\PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{pipeline} \PY{k}{import} \PY{n}{make\PYZus{}pipeline}


\PY{k}{def} \PY{n+nf}{f}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} function to approximate by polynomial interpolation\PYZdq{}\PYZdq{}\PYZdq{}}
    \PY{k}{return} \PY{n}{x} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{x}\PY{p}{)}


\PY{c+c1}{\PYZsh{} generate points used to plot}
\PY{n}{x\PYZus{}plot} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}

\PY{c+c1}{\PYZsh{} generate points and keep a subset of them}
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{100}\PY{p}{)}
\PY{n}{rng} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{RandomState}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{rng}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{x}\PY{p}{)}
\PY{n}{x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sort}\PY{p}{(}\PY{n}{x}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{20}\PY{p}{]}\PY{p}{)}
\PY{n}{y} \PY{o}{=} \PY{n}{f}\PY{p}{(}\PY{n}{x}\PY{p}{)}

\PY{c+c1}{\PYZsh{} create matrix versions of these arrays}
\PY{n}{X} \PY{o}{=} \PY{n}{x}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}
\PY{n}{X\PYZus{}plot} \PY{o}{=} \PY{n}{x\PYZus{}plot}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{]}

\PY{n}{colors} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{red}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{teal}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{yellowgreen}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gold}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
\PY{n}{lw} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}plot}\PY{p}{,} \PY{n}{f}\PY{p}{(}\PY{n}{x\PYZus{}plot}\PY{p}{)}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cornflowerblue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{n}{lw}\PY{p}{,}
         \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ground truth}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{navy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{30}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{training points}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}

\PY{k}{for} \PY{n}{count}\PY{p}{,} \PY{n}{degree} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{4}\PY{p}{,} \PY{l+m+mi}{5}\PY{p}{]}\PY{p}{)}\PY{p}{:}
    \PY{n}{model} \PY{o}{=} \PY{n}{make\PYZus{}pipeline}\PY{p}{(}\PY{n}{PolynomialFeatures}\PY{p}{(}\PY{n}{degree}\PY{p}{)}\PY{p}{,} \PY{n}{Ridge}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{model}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
    \PY{n}{y\PYZus{}plot} \PY{o}{=} \PY{n}{model}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{X\PYZus{}plot}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}plot}\PY{p}{,} \PY{n}{y\PYZus{}plot}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n}{colors}\PY{p}{[}\PY{n}{count}\PY{p}{]}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{n}{lw}\PY{p}{,}
             \PY{n}{label}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{degree }\PY{l+s+si}{\PYZpc{}d}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{n}{degree}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lower left}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Automatically created module for IPython interactive environment
\end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Basics_of_Machine_Learning_part2_files/Basics_of_Machine_Learning_part2_14_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{classification}{%
\subsection{Classification}\label{classification}}

When we have \emph{categorical responses} (e.g., ``high'',
``low'',``medium'') the our prediction problem become a
\textbf{classification problem}. Let's suppose that we are looking for a
estimate \(f\) basis of the \emph{training} observations
\(\{ (x_1,y_1), \ldots, (x_n,y_n)\}\) where \(y_i\) is a member of one
given class \(C_k\) (\(k\in \{1,\ldots,r\}\).

One common approach to quantify the quality of the estimate \(\hat{f}\)
is the \textbf{training error rate}, that is the proportion of
classification errors that we incurred during if we apply our estimate
\(\hat{f}\).

\begin{equation}
\frac{1}{n} \sum_{i=1}^n I_{y_i \ne \hat{y}_i}
\end{equation}

where \(\hat{y}_i\) is our prediction for the observation \(i\) by the
estimate \(\hat{f}\), and \(I_{y_i \ne \hat{y}_i}\) is the
\emph{indicator function} which is equal to 1 if the condition is true
(i.e. \(y_i \ne \hat{y}_i\)) and zero elsewhere. Thus the expression on
the right is measuring the fraction of incorrect classifications.

As we did before, we can define the \textbf{test error rate} associated
with a set of previously unseen observations \((x_0,y_0)\) as

\begin{equation}
\mathrm{Average}(I_{y_0 \ne \hat{y}_0}) 
\label{eq:classtesterror}
\end{equation}

\hypertarget{bayes-classifier}{%
\subsubsection{Bayes classifier}\label{bayes-classifier}}

The test error rate given in (\ref{eq:classtesterror}) can be minimized
(on average) by a simple classifier that assigns \emph{each observation
the most likely class, given its predictor values}). So, for the test
observation with predictor vector \(x_0\) will belong to the class \(j\)
for which

\begin{equation*}
\mathrm{Pr}(Y=j | X=x_0)
\end{equation*}

is largest. This classifier is refered to as \textbf{Bayes classifier}.

The Bayes classifier produces the lowest possible test error rate,
calles the \textbf{Bayes error rate}. In general the overall Bayes error
rate is given by

\begin{equation*}
1 - E \left( \max_{j} \mathrm{Pr}(Y=j|X) \right)
\end{equation*}

\hypertarget{k-nearest-neighbors}{%
\subsubsection{K-Nearest Neighbors}\label{k-nearest-neighbors}}

One of the problems with the Bayes classifier is that we normally don't
know the conditional distribution \(Y\) given \(X\). Thus, it becomes a
\emph{theoretical gold standard} which is unattainable.

\(K\)-nearest neighbors (KNN) in which given a positive integer \(K\)
and a test observation \(x_0\), the \emph{KNN} classifier first
identifies the \(K\) points in the trianing data that are the closests
to \(x_0\), represented by \(\cal{N}_0\). It then estimates the
conditional probability for class \(j\) as the fraction of points in
\(\cal{N}_0\) whose response values equal \(j\):

\begin{equation*}
\mathrm{Pr}(Y=j | X=x_0) = \frac{1}{K} \sum_{i \in {\cal N}_0} I_{y_i=j}
\end{equation*}

Finally, KNN aplies Bayes rule and classifies the test observation to
\(x_0\) to the class with the largest probability.

    \hypertarget{example}{%
\paragraph{Example}\label{example}}

Let's suppose we have two classes \(\circ\) of two colors and \(\times\)
represents the center of an unseen new point \(x_0\). And let's pick
\(K=3\). So the closests three points are inside or in the boundary of
the green circle. This circle has 2 blue and 1 orange point. Thus,
resulting in estimated probabilities of \(2/3\) for the blue and \(1/3\)
for the orange class.


\begin{figure}[h!]
    \centering
    \includegraphics{knn_2d.png}
 \end{figure}

Then, the \(\times\) point belongs to the blue class. When the same
procedure is applied to a sufficient number of points in the plane, we
get something like this

\begin{figure}[h!]
    \centering
    \includegraphics{knn_2dbis.png}
\end{figure}

    \hypertarget{effect-of-k}{%
\subsubsection{\texorpdfstring{Effect of
\(K\)}{Effect of K}}\label{effect-of-k}}

The choice of \(K\) has a significant impact on the KNN classifier.

\begin{figure}[h!]
    \centering
    \includegraphics{knn_k_comparison.png}
    \caption{KNN classification pattern changes with the value of K}
\end{figure}

From this picture, we can see that when \(K=1\), the \emph{decision
boundary} is overly flexible (i.e.~almost classifies the dataset
``perfectly''). This case we expect that the classifier has low bias but
very high variance. On the other hand, when we have a high \(K\) (in the
picture \(K=100\)) we get almost linear classification, where low
variance and high bias is expected.


\begin{figure}[h!]
    \centering
    \includegraphics{knn_flexibility.png}
    \caption{Training and test errors based on the $1/K$ variable}
\end{figure}
Just as we did with the other classifiers, we consider the variable
\(1/K\) as \emph{flexibility}

Thus, we observe the same \(U\) shape for the \emph{test error rate} and
decreasing \emph{training error rate} as the model tends to overfit
data. The dotted line represents the Bayes error rate.

For the case of \(K=10\) we have

\begin{figure}[h!]
    \centering
    \includegraphics{knn_k_optimal.png}
    \caption{``Optimal'' $K=10$ for the classification}
\end{figure}


    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\hspace{4pt}}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{from} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{colors} \PY{k}{import} \PY{n}{ListedColormap}
\PY{k+kn}{from} \PY{n+nn}{sklearn} \PY{k}{import} \PY{n}{neighbors}\PY{p}{,} \PY{n}{datasets}

\PY{n}{n\PYZus{}neighbors} \PY{o}{=} \PY{l+m+mi}{15}

\PY{c+c1}{\PYZsh{} import some data to play with}
\PY{n}{iris} \PY{o}{=} \PY{n}{datasets}\PY{o}{.}\PY{n}{load\PYZus{}iris}\PY{p}{(}\PY{p}{)}

\PY{c+c1}{\PYZsh{} we only take the first two features. We could avoid this ugly}
\PY{c+c1}{\PYZsh{} slicing by using a two\PYZhy{}dim dataset}
\PY{n}{X} \PY{o}{=} \PY{n}{iris}\PY{o}{.}\PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{p}{:}\PY{l+m+mi}{2}\PY{p}{]}

\PY{n}{y} \PY{o}{=} \PY{n}{iris}\PY{o}{.}\PY{n}{target}

\PY{n}{h} \PY{o}{=} \PY{o}{.}\PY{l+m+mi}{02}  \PY{c+c1}{\PYZsh{} step size in the mesh}

\PY{c+c1}{\PYZsh{} Create color maps}
\PY{n}{cmap\PYZus{}light} \PY{o}{=} \PY{n}{ListedColormap}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cyan}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{cornflowerblue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
\PY{n}{cmap\PYZus{}bold} \PY{o}{=} \PY{n}{ListedColormap}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkorange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{c}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{darkblue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}

\PY{k}{for} \PY{n}{weights} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{uniform}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{distance}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} we create an instance of Neighbours Classifier and fit the data.}
    \PY{n}{clf} \PY{o}{=} \PY{n}{neighbors}\PY{o}{.}\PY{n}{KNeighborsClassifier}\PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{p}{,} \PY{n}{weights}\PY{o}{=}\PY{n}{weights}\PY{p}{)}
    \PY{n}{clf}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Plot the decision boundary. For that, we will assign a color to each}
    \PY{c+c1}{\PYZsh{} point in the mesh [x\PYZus{}min, x\PYZus{}max]x[y\PYZus{}min, y\PYZus{}max].}
    \PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{y\PYZus{}min}\PY{p}{,} \PY{n}{y\PYZus{}max} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{l+m+mi}{1}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{1}
    \PY{n}{xx}\PY{p}{,} \PY{n}{yy} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{meshgrid}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{x\PYZus{}min}\PY{p}{,} \PY{n}{x\PYZus{}max}\PY{p}{,} \PY{n}{h}\PY{p}{)}\PY{p}{,}
                         \PY{n}{np}\PY{o}{.}\PY{n}{arange}\PY{p}{(}\PY{n}{y\PYZus{}min}\PY{p}{,} \PY{n}{y\PYZus{}max}\PY{p}{,} \PY{n}{h}\PY{p}{)}\PY{p}{)}
    \PY{n}{Z} \PY{o}{=} \PY{n}{clf}\PY{o}{.}\PY{n}{predict}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{c\PYZus{}}\PY{p}{[}\PY{n}{xx}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{yy}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{]}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Put the result into a color plot}
    \PY{n}{Z} \PY{o}{=} \PY{n}{Z}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{pcolormesh}\PY{p}{(}\PY{n}{xx}\PY{p}{,} \PY{n}{yy}\PY{p}{,} \PY{n}{Z}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{cmap\PYZus{}light}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Plot also the training points}
    \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{X}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{c}\PY{o}{=}\PY{n}{y}\PY{p}{,} \PY{n}{cmap}\PY{o}{=}\PY{n}{cmap\PYZus{}bold}\PY{p}{,}
                \PY{n}{edgecolor}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{k}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{s}\PY{o}{=}\PY{l+m+mi}{20}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{xx}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{xx}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{ylim}\PY{p}{(}\PY{n}{yy}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{yy}\PY{o}{.}\PY{n}{max}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{3\PYZhy{}Class classification (k = }\PY{l+s+si}{\PYZpc{}i}\PY{l+s+s2}{, weights = }\PY{l+s+s2}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s2}{\PYZsq{}}\PY{l+s+s2}{)}\PY{l+s+s2}{\PYZdq{}}
              \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{n\PYZus{}neighbors}\PY{p}{,} \PY{n}{weights}\PY{p}{)}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Basics_of_Machine_Learning_part2_files/Basics_of_Machine_Learning_part2_18_0.png}
    \end{center}
    { \hspace*{\fill} \\}
   
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Basics_of_Machine_Learning_part2_files/Basics_of_Machine_Learning_part2_18_1.png}
    \end{center}
 

 %  { \hspace*{\fill} \\}
    
 \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
  Some of the figures and material in this presentation are taken from
``An Introduction to Statistical Learning, with applications in R''
(Springer, 2013) with permission from the authors: G. James, D. Witten,
T. Hastie and R. Tibshirani
\end{tcolorbox}
    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
